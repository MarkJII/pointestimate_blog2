---
title: "Sample Size and Percentages: What to Expect"
author: "Mark Jurries"
date: 2021-11-15T08:00:00-05:00
categories: ["probability"]
tags: ["probability", "plot", "r"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(tidyverse)
library(hrbrthemes)
library(ggridges)
library(gt)
library(ggtext)

set.seed(2021)
```
People are really good at comparing numbers. If you have 1,000 apples and I have 10, you can tell me pretty quickly that you have 990 more apples than I do. You can also put that into some context - we know intuitively that that's a *lot* of apples - so it's helpful as well.

Percentages are numbers as well, so they should be easy to measure as well. If Jack takes part in a Euchre tournament wins 60% of his games, then he's pretty good. Right?

Well, maybe. Firstly, we need to define how we're looking at that number. If we're simply asking how Jack played, then we use the 60% as what we call a *descriptive* statistic. That is, it tells us a story about what happened. If we want to use it to measure how we think he'll do in the future, then it becomes a *predictive* statistic, and its usefulness will depend on how many games he played. 

In this case, it turns out he played 10. Knowing that, we can calculate what's called the Standard Error. "Error" in this case doesn't mean a mistake, rather, it refers to range we expect for the win percentage. If you've ever seen a poll with a +/- number, that's what this is - they expect that between *x* and *y* percent of the population agree with whatever they're polling about.

> **Boring Mathy Stuff** If you don't care to know the math, feel free to skip ahead. The standard error is defined as:
> <br> <br>sqrt(Win Percentage * (1 - Win Percentage) / Games )
> <br><br>If we plug in Jack's numbers, we get:
> <br><br>sqrt((.6 * .4) / 10)
> <br>sqrt(0.24) / 10)
> <br>sqrt(0.024)
> <br>0.015

We can then use the standard error along with the win percentage to get our range of what we think his talent really is.
```{r ten_game_data, echo = FALSE, warning = FALSE, message = FALSE}

euchre_results <- tibble(Games = c(10), 
                         Wins = c(6)) %>%
  mutate(Win_Percentage = Wins/Games,
         Standard_Error = sqrt(Win_Percentage * (1-Win_Percentage)/Games),
         Lower_Range = Win_Percentage - Standard_Error,
         Higher_Range = Win_Percentage + Standard_Error)

euchre_results %>%
  gt() %>%
  fmt_percent(
      columns = c(Win_Percentage, Standard_Error, Lower_Range, Higher_Range),
      decimals = 1
      ) %>%
  tab_options(
      column_labels.border.top.color = "white",
      column_labels.border.top.width = px(3),
      column_labels.border.bottom.color = "black",
      table_body.hlines.color = "white",
      table.border.bottom.color = "white",
      table.border.bottom.width = px(3)
      ) %>%
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_text(weight = "bold")
      )
    ) %>% 
    cols_label(
      Win_Percentage = "Win Percentage",
      Standard_Error = "Standard Error",
      Lower_Range = "Lower Range",
      Higher_Range = "Higher Range"
      )
```
So based on 10 games, we expect that if Jack played 1,000 games, his winning percentage would be somewhere between 44.5% and 77.5%. This makes sense - 60% isn't that far off from 50%, which is just a coin flip. It's more, though, so we think he's probably a winning player, although we still think there's a chance he's not.

>**Technical aside:** In a real-world situation, we'd look at a lot more data - such as the winning percentage of all Euchre players, quality of opponents, quality of partners, etc. And if most players fell in a lesser range - say 40% to 50% - we'd apply additional fancy math to bring our expectations in line with the general populace. We're going to gloss over that here, just know there are other ways of estimating talent.

We know intuitively that 10 games isn't a lot. So we have another tournament (it's the Midwest, after all, these things happen all the time) and Jack plays another 10 games. Conveniently for our illustration, he wins exactly six more, so now we can simply double our totals. Let's see what that does to our estimates:

```{r twenty_game_data, echo = FALSE, warning = FALSE, message = FALSE}
euchre_results <- tibble(Games = c(20), 
                         Wins = c(12)) %>%
  mutate(Win_Percentage = Wins/Games,
         Standard_Error = sqrt(Win_Percentage * (1-Win_Percentage)/Games),
         Lower_Range = Win_Percentage - Standard_Error,
         Higher_Range = Win_Percentage + Standard_Error)

euchre_results %>%
  gt() %>%
  fmt_percent(
      columns = c(Win_Percentage, Standard_Error, Lower_Range, Higher_Range),
      decimals = 1
      ) %>%
  tab_options(
      column_labels.border.top.color = "white",
      column_labels.border.top.width = px(3),
      column_labels.border.bottom.color = "black",
      table_body.hlines.color = "white",
      table.border.bottom.color = "white",
      table.border.bottom.width = px(3)
      ) %>%
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_text(weight = "bold")
      )
    ) %>% 
    cols_label(
      Win_Percentage = "Win Percentage",
      Standard_Error = "Standard Error",
      Lower_Range = "Lower Range",
      Higher_Range = "Higher Range"
      )

```
Now that we have more information our standard error has shrunk. We think it a lot less likely that he's a losing player, although we also think it's less likely that he's a superstar. That said, going +/- 11% is still not super-helpful. Let's see how that changes as we add more games to Jack's totals.

```{r jack_game_data, echo = FALSE, warning = FALSE, message = FALSE}
euchre_results <- tibble(Games = seq(10, 100, by = 10), 
                         Wins = seq(6, 60, by = 6)) %>%
  mutate(Win_Percentage = Wins/Games,
         Standard_Error = sqrt(Win_Percentage * (1-Win_Percentage)/Games),
         Lower_Range = Win_Percentage - Standard_Error,
         Higher_Range = Win_Percentage + Standard_Error)

euchre_results %>%
  gt() %>%
  fmt_percent(
      columns = c(Win_Percentage, Standard_Error, Lower_Range, Higher_Range),
      decimals = 1
      ) %>%
  tab_options(
      column_labels.border.top.color = "white",
      column_labels.border.top.width = px(3),
      column_labels.border.bottom.color = "black",
      table_body.hlines.color = "white",
      table.border.bottom.color = "white",
      table.border.bottom.width = px(3)
      ) %>%
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_text(weight = "bold")
      )
    ) %>% 
    cols_label(
      Win_Percentage = "Win Percentage",
      Standard_Error = "Standard Error",
      Lower_Range = "Lower Range",
      Higher_Range = "Higher Range"
      )

```
The more games we add, the more we think it likely that Jack's true talent is around 60%. Now, in this example, he's Mr. Consistent, winning 6 out of every 10 games no matter what, so we're in no duh, Sherlock territory. Let's take a look at a slightly more realistic scenario, this time envisioning what his winning percentage is every ten games. To do this, we'll simulate the data - imagine Dr. Strange looping Dormammu in a time loop, only in this case we're creating a digital copy of Jack and making him play Euchre over and over again, each time with slightly different results. (No Jacks were harmed in the making of this data.)

```{r simmed_games, echo = FALSE, warning = FALSE, message = FALSE}
sample_size <- 600
sample_rate <- .6

sim_seq <- tibble(rep = 1:1000) %>% 
  mutate(simdata = map(rep, ~rbinom(sample_size, 1, sample_rate))) %>%
  unnest() %>%
  group_by(rep) %>%
  mutate(running_mean = cummean(simdata),
         n = row_number(),
         running_se = (running_mean * (1 - running_mean) / n) ^ .5,
         running_low = running_mean - (running_se),
         running_high = running_mean + (running_se)) %>%
  ungroup() %>%
  group_by(n) %>%
  mutate(mean_sim = mean(running_mean))

sim_one <- sim_seq %>%
  filter(rep == 8 & n%%10 == 0 & n <= 100)

sim_one %>%
  ungroup() %>%
  select(n, running_mean, running_se, running_low, running_high) %>%
  gt() %>%
    fmt_percent(
      columns = c(running_mean, running_se, running_low, running_high),
      decimals = 1
      ) %>%
  tab_options(
      column_labels.border.top.color = "white",
      column_labels.border.top.width = px(3),
      column_labels.border.bottom.color = "black",
      table_body.hlines.color = "white",
      table.border.bottom.color = "white",
      table.border.bottom.width = px(3)
      ) %>%
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_text(weight = "bold")
      )
    ) %>% 
    cols_label(
      n = "Games",
      running_mean = "Win Percentage",
      running_se = "Standard Error",
      running_low = "Lower Range",
      running_high = "Higher Range"
      )
```
In this example, Jack hit 100 games with a 57% win percentage, but we expect his him to be between 52% and 62% going forward. But since we're making our virtual Jack play over and over again, let's take a look at another set of 100 games.

>**Technical aside:** What we're now looking at is how many games we expect someone with a true 60% win talent to perform in a random set of 100 games. We're looking to illustrate something else, but just bear in mind we're hijacking one process to show another.

```{r simmed_table_two, echo = FALSE, warning = FALSE, message = FALSE}

sim_two <- sim_seq %>%
  filter(rep == 2 & n%%10 == 0 & n <= 100)

sim_two %>%
  ungroup() %>%
  select(n, running_mean, running_se, running_low, running_high) %>%
  gt() %>%
    fmt_percent(
      columns = c(running_mean, running_se, running_low, running_high),
      decimals = 1
      ) %>%
  tab_options(
      column_labels.border.top.color = "white",
      column_labels.border.top.width = px(3),
      column_labels.border.bottom.color = "black",
      table_body.hlines.color = "white",
      table.border.bottom.color = "white",
      table.border.bottom.width = px(3)
      ) %>%
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_text(weight = "bold")
      )
    ) %>% 
    cols_label(
      n = "Games",
      running_mean = "Win Percentage",
      running_se = "Standard Error",
      running_low = "Lower Range",
      running_high = "Higher Range"
      )
```
Well, this one looks more promising, anyway - 64% success rate, and we think he's at least a 59% player. But our last sim was at 57%, so we had an 8 point swing. This shows why sample size is so critical - we can get very different numbers within even 100 games, even though in both cases he's a 60% winner. 

To get a better feel for this, let's make our virtual Jack play 100 games of Euchre 1,000 times. We can then see how much his win % varies by looking at all of our sims at once. The blue line is the average of all our sims put together.

```{r simmed_table_plot, echo = FALSE, warning = FALSE, message = FALSE, fig.width= 10, fig.height = 4}
sim_seq %>%
  filter(n <= 100) %>%
  ggplot(aes(x = n, y = running_mean, group = rep))+
  geom_line(alpha = 0.7, size = 0.2, color = '#bcc0cc')+
  geom_line(aes(x = n, y = mean_sim), color = '#6a84d4')+
  theme_ipsum()+
  xlab("Games Played")+
  ylab("Win Percentage")+
  scale_y_continuous(labels = scales::percent)
```
Notice how our range gets smaller the more we play, and how our simulated game range matches closely with our standard error. This drives in our key takeaway: sample sizes always have some randomness, even at what most of use would consider to be a large amount. (We think 100 is large, but in this context it's not as large as we'd like). So when you see a percentage and want to predict what it will do in the future, do so knowing that the amount of data is meaningful as well for making that prediction.

**Appendix:** We can see how the standard error changes with both the success rate and sample size. Note it's largest at 50%, since that's our standard coin toss. Also note that it's symeterical - 0.9 and 0.1 have the same values. Since the formula uses success rate * (1-success rate), then 0.9 * 0.1 and 0.1 * 0.9 have the same value. This is also why 0 and 1 have no error. 1 * 0 is zero.

```{r both_simmed, echo = FALSE, warning = FALSE, message = FALSE}
sample_size <- seq(50, 600, by = 50)
success_rate <- seq(0, 1, by = 0.05)

se_table <- sample_size %>%
  as_tibble() %>%
  rename(sample_size = value) %>%
  crossing(success_rate) %>%
  mutate(standard_error = sqrt(success_rate * (1 - success_rate) / sample_size))

se_grid <- se_table %>%
  pivot_wider(names_from = sample_size, values_from = standard_error)

se_grid %>%
  gt() %>%
  fmt_percent(
    columns = everything(),
    decimals = 1
  ) %>%
  tab_options(
    column_labels.border.top.color = "white",
    column_labels.border.top.width = px(3),
    column_labels.border.bottom.color = "black",
    table_body.hlines.color = "white",
    table.border.bottom.color = "white",
    table.border.bottom.width = px(3)
  ) %>%
  tab_style(
    locations = cells_column_labels(columns = everything()),
    style = list(
      cell_text(weight = "bold")
    )
  ) %>%
  cols_label(
    success_rate = "Success Rate"
  ) %>%
  data_color(
    columns = 2:13,
    colors = scales::col_numeric(
      palette = 'Blues',
      domain = c(0, 0.1))
  )
```